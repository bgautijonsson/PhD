[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Improved 21st century projections of sub-daily extreme precipitation by spatio-temporal recalibration",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Projects/Modeling/Max & Smooth/index.html",
    "href": "Projects/Modeling/Max & Smooth/index.html",
    "title": "1  Max & Smooth",
    "section": "",
    "text": "Code\nd <- here(\"Data\", \"yearly_maximum_per_hour.csv\") |> \n    read_csv() |> \n    filter(year <= 2040, proj_x <= 10, proj_y <= 10) |> \n    arrange(station, year)"
  },
  {
    "objectID": "Projects/Modeling/Max & Smooth/index.html#fit-gev-with-trend",
    "href": "Projects/Modeling/Max & Smooth/index.html#fit-gev-with-trend",
    "title": "1  Max & Smooth",
    "section": "1.2 Fit GEV with trend",
    "text": "1.2 Fit GEV with trend\n\n\nCode\nlog_lik_trend <- function(dat, par) {\n    \n    y <- dat$y\n    t <- dat$year\n    t <- t - 1981\n    \n    mu0 <- exp(par[1])\n    sigma <- exp(par[2]) * mu0\n    xi <- 1 / (1 + exp(-par[3])) - 0.5\n    delta <- 0.016 * 1 / (1 + exp(-par[4])) - 0.008\n    \n    mu <- mu0 * (1 + delta * t)\n    \n    m <- length(y)\n    \n    z <- (y - mu) / sigma\n    \n    if (any(1 + xi * z <= 0)) return(NA)\n    \n    \n    out <- - m * log(sigma) \n    out <- out - (1 + 1/xi) * sum(log(1 + xi * z))\n    out <- out - sum((1 + xi * z)^{-1/xi})\n    \n    priors <- dnorm(par[1], mean = 2.4, sd = 1, log = T) + \n        dnorm(par[2], mean = -0.5, sd = 1, log = T) + \n        dnorm(par[3], mean = 0, sd = 1, log = T) + \n        dnorm(par[4], mean = 0, sd = 1, log = T)\n    \n    out <- out + priors\n    \n    -out\n}\n\n\n\n\nCode\nf_trend <- function(x, t, mu0, sigma, xi, delta) {\n    t <- t - 1981\n    mu <- mu0 * (1 + delta * t)\n    exp(-(1 + xi * (x - mu) / sigma)^{-1/xi})\n}\n\nfit_gev_trend <- function(data, ...) {\n    \n    y <- data$y\n    \n    opt <- optim(log_lik_trend,\n                 par = c(0, 0, 0.1, 0), \n                 dat = data,\n                 hessian = T)\n    \n    par <- opt$par\n    mu <- exp(par[1])\n    sigma <- exp(par[2]) * mu\n    xi <- 1 / (1 + exp(-par[3])) - 0.5\n    delta <- 0.016 * 1 / (1 + exp(-par[4])) - 0.008\n    \n    hess <- opt$hessian \n    \n    tibble(hess = list(hess),\n           par = list(par))\n    \n}\n\n\n\n\nCode\nd_fit <- d |> \n    filter(proj_x <= 10, proj_y <= 10) |>\n    select(year, y = precip, station, proj_x, proj_y) |> \n    group_by(station, proj_x, proj_y) |> \n    group_modify(fit_gev_trend) |> \n    ungroup()\n\n\n\n\nCode\neta_hat <- Matrix(unlist(d_fit$par), ncol = 1)\nQ_etay <- bdiag(d_fit$hess)\nchol_Q_etay <- bdiag(d_fit$hess |> map(chol))\n\ndim(eta_hat)\n\n\n[1] 400   1\n\n\nCode\ndim(Q_etay)\n\n\n[1] 400 400\n\n\nCode\ndim(chol_Q_etay)\n\n\n[1] 400 400\n\n\n\n\nCode\neta <- Matrix(0,\n              nrow = nrow(eta_hat),\n              ncol = 1)\nZ <- Matrix(data = 0, \n            nrow = nrow(eta_hat),\n            ncol = 4)\n\nfor (i in 1:4) Z[i + 0:99 * 4, i] <- 1\n\nnu <- Matrix(data = 0,\n             nrow = 4,\n             ncol = 1)"
  },
  {
    "objectID": "Projects/Modeling/Max & Smooth/index.html#test-run",
    "href": "Projects/Modeling/Max & Smooth/index.html#test-run",
    "title": "1  Max & Smooth",
    "section": "1.3 Test run",
    "text": "1.3 Test run\n\n\nCode\npriors <- d_fit |> \n    unnest(par) |> \n    group_by(station) |> \n    mutate(term = row_number()) |> \n    ungroup() |> \n    group_by(term) |> \n    summarise(mean = mean(par),\n              sd = sd(par),\n              .groups = \"drop\")\n\n\n\n\nCode\ny <- eta_hat\nx <- nu\n\nQ_e <- Q_etay\n\nZ <- Matrix(data = 0, \n            nrow = nrow(eta_hat),\n            ncol = 4)\n\nfor (i in 1:4) Z[i + 0:99 * 4, i] <- 1\n\nmu_x <- Matrix(priors$mean)\nQ_x <- .sparseDiagonal(n = nrow(mu_x), x = 1 / priors$sd^2)\nchol_Q_x <- Cholesky(Q_x)\n\n\nyx <- rbind(y, x)\n\nQ_yx_cond_theta <- Matrix(data = 0,\n                          nrow = nrow(y) + nrow(x),\n                          ncol = nrow(y) + nrow(x))\n\n# Upper left of matrix\nQ_yx_cond_theta[1:nrow(y), 1:nrow(y)] <- Q_e\n# Upper right of matrix\nQ_yx_cond_theta[1:nrow(y), (nrow(y) + 1):(nrow(y) + nrow(x))] <- -Q_e %*% Z\n# Lower left of matrix\nQ_yx_cond_theta[(nrow(y) + 1):(nrow(y) + nrow(x)), 1:nrow(y)] <- - t(Z) %*% Q_e\n# Lower right of matrix\nQ_yx_cond_theta[(nrow(y) + 1):(nrow(y) + nrow(x)), (nrow(y) + 1):(nrow(y) + nrow(x))] <- Q_x + t(Z) %*% Q_e %*% Z\n\nchol_Q_yx_cond_theta <- Cholesky(Q_yx_cond_theta)\n\n\nQ_x_cond_y <- Q_x + t(Z) %*% Q_e %*% Z\nmu_x_cond_y <- solve(Q_x_cond_y, Q_x %*% mu_x + t(Z) %*% Q_e %*% y)"
  },
  {
    "objectID": "Projects/Modeling/Max & Smooth/index.html#sampling-from-the-posterior",
    "href": "Projects/Modeling/Max & Smooth/index.html#sampling-from-the-posterior",
    "title": "1  Max & Smooth",
    "section": "1.4 Sampling from the posterior",
    "text": "1.4 Sampling from the posterior\n\n1.4.1 1. Sample \\(\\theta\\) from marginal\n\\[\n\\pi(\\theta \\vert \\hat\\eta) \\propto \\pi(\\theta) \\frac{\\pi(\\hat\\eta\\vert x, \\theta)\\pi(x\\vert\\theta)}{\\pi(x\\vert\\hat\\eta, \\theta)}\n\\]\nWe need to know three conditional distributions:\n\n1.4.1.1 1. $p(x |, y)\n\\[\np(x | \\theta, y) = N(x | \\mu_{x|y}, Q_{x|y})\n\\]\n\\[\n\\mu_{x|y} = Q^{-1}_{x|y}(Q_x\\mu_x + Z^TQ_\\varepsilon y) \\\\\nQ_{x|y} = Q_x + Z^TQ_\\varepsilon Z\n\\]\nIn code, we get:\n\n\nCode\nQ_x_cond_y <- Q_x + t(Z) %*% Q_e %*% Z\nmu_x_cond_y <- solve(Q_x_cond_y, Q_x %*% mu_x + t(Z) %*% Q_e %*% y)\n\nchol_Q_x_cond_y <- Cholesky(Q_x_cond_y)\n\npi_x_cond_theta_y <- function(x, mu_x_cond_y, chol_Q_x_cond_y) {\n    dmvn.sparse(\n        x = t(x),\n        mu = as.vector(mu_x_cond_y),\n        CH = chol_Q_x_cond_y,\n        prec = TRUE,\n        log = TRUE\n    )\n}\n\npi_x_cond_theta_y(x = x, mu_x_cond_y = mu_x_cond_y, chol_Q_x_cond_y = chol_Q_x_cond_y)\n\n\n[1] -106542.4\n\n\n\n\n1.4.1.2 2. \\(p(y |x, \\theta)\\)\n\\[\np(y | x, \\theta) = N(y | Zx, Q_\\varepsilon^{-1})\n\\]\nIn code, we get:\n\n\nCode\nQ_e <- bdiag(d_fit$hess)\nchol_Q_e <- Cholesky(Q_e)\n\n\npi_y_cond_x_theta <- function(y, Z, x, chol_Q_e) {\n    dmvn.sparse(\n        x = t(y), \n        mu = as.vector(Z %*% x), \n        CH = chol_Q_e,\n        prec = TRUE,\n        log = TRUE\n    )\n}\n\npi_y_cond_x_theta(y = y, Z = Z, x = x, chol_Q_e = chol_Q_e)\n\n\n[1] -105787.3\n\n\n\n\n1.4.1.3 3. \\(p(x | \\theta)\\)\n\\[\np(x | \\theta)  = N(x | \\mu_x, Q_x^{-1})\n\\]\n\n\nCode\npi_x_cond_theta <- function(x, mu_x, chol_Q_x) {\n    dmvn.sparse(x = t(x),\n                mu = as.vector(mu_x),\n                CH = chol_Q_x,\n                prec = TRUE,\n                log = TRUE)\n}\n\npi_x_cond_theta(x = x, mu_x = mu_x, chol_Q_x = chol_Q_x)\n\n\n[1] -659.8415\n\n\n\n\nCode\npi_theta <- function(theta) 1\npi_theta_cond_y <- function(theta, y, x, mu_x, chol_Q_x, Z, chol_Q_e,\n                            mu_x_cond_y, chol_Q_x_cond_y) {\n    pi_theta(theta) * \n        pi_y_cond_x_theta(y = y, Z = Z, x = x, chol_Q_e = chol_Q_e) * \n        pi_x_cond_theta(x = x, mu_x = mu_x, chol_Q_x = chol_Q_x) /\n        pi_x_cond_theta_y(x = x, mu_x_cond_y = mu_x_cond_y, chol_Q_x_cond_y = chol_Q_x_cond_y)\n}\n\npi_theta_cond_y(\n    theta = theta, \n    y = y,\n    x = x, \n    mu_x = mu_x, \n    chol_Q_x = chol_Q_x, \n    Z = Z,\n    chol_Q_e, \n    mu_x_cond_y,\n    chol_Q_x_cond_y = chol_Q_x_cond_y\n)\n\n\n[1] -655.1651\n\n\n\n\n\n1.4.2 2. Sample \\(\\mu_x\\) and \\(Q_x\\) given \\(\\theta\\) and \\(\\hat\\eta\\)\n\n\nCode\nQ_x_cond_y <- Q_x + t(Z) %*% Q_e %*% Z\nchol_Q_x_cond_y <- Cholesky(Q_x_cond_y)\n\nmu_x_cond_y <- solve(Q_x_cond_y, Q_x %*% mu_x + t(Z) %*% Q_e %*% y)\n\n\nrmvn.sparse(\n    n = 1,\n    mu = mu_x_cond_y,\n    CH = chol_Q_x_cond_y,\n    prec = TRUE\n)\n\n\n1 x 4 Matrix of class \"dgeMatrix\"\n         [,1]      [,2]       [,3]      [,4]\n[1,] 2.512701 -1.018856 -0.3164546 0.3558238"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Improved 21st century projections of sub-daily extreme precipitation by spatio-temporal recalibration",
    "section": "",
    "text": "This online book contains my notes and computations for my PhD project titled Improved 21st century projections of sub-daily extreme precipitation by spatio-temporal recalibration."
  },
  {
    "objectID": "Projects/Modeling/Max & Smooth/index.html",
    "href": "Projects/Modeling/Max & Smooth/index.html",
    "title": "1  Max & Smooth",
    "section": "",
    "text": "We read in data from 100 of the geographical locations from 1980 to 2014\n\n\nCode\nd <- here(\"Data\", \"yearly_maximum_per_hour.csv\") |> \n    read_csv() |> \n    filter(year <= 2040, proj_x <= 10, proj_y <= 10) |> \n    arrange(station, year)\n\n\n\n\nCode\nd |> \n    filter(proj_x <= 3, proj_y <= 3) |> \n    ggplot(aes(year, precip)) +\n    geom_point() +\n    facet_grid(proj_x ~ proj_y, labeller = labeller(.rows = label_both, .cols = label_both)) +\n    labs(x = NULL,\n         y = NULL) +\n    theme_half_open()\n\n\n\n\n\nFigure 1. Modelled maximum hourly precipitation for nine stations"
  },
  {
    "objectID": "Projects/Modeling/Max & Smooth/index.html#fit-gev-with-trend",
    "href": "Projects/Modeling/Max & Smooth/index.html#fit-gev-with-trend",
    "title": "1  Max & Smooth",
    "section": "1.2 Fit GEV with trend",
    "text": "1.2 Fit GEV with trend\nDefine the Generalized Extreme Value distribution as\n\\[\n\\mathrm{GEV}(y \\vert \\mu, \\sigma, \\xi) = \\begin{cases}\ne^{-\\left(1 + \\xi\\frac{y - \\mu}{\\sigma}\\right)^{-1/\\xi}_+}, \\xi \\neq 0; \\\\\ne^{-e^{-\\frac{y - \\mu}{\\sigma}}}, \\xi = 0.\n\\end{cases}\n\\]\nWith the addition of a trend parameter, \\(\\Delta\\), this becomes\n$$\n$$\n\n\nCode\nlog_lik_trend <- function(dat, par) {\n    \n    y <- dat$y\n    t <- dat$year\n    t <- t - 1981\n    \n    mu0 <- exp(par[1])\n    sigma <- exp(par[2]) * mu0\n    xi <- 1 / (1 + exp(-par[3])) - 0.5\n    delta <- 0.016 * 1 / (1 + exp(-par[4])) - 0.008\n    \n    mu <- mu0 * (1 + delta * t)\n    \n    m <- length(y)\n    \n    z <- (y - mu) / sigma\n    \n    if (any(1 + xi * z <= 0)) return(NA)\n    \n    \n    out <- - m * log(sigma) \n    out <- out - (1 + 1/xi) * sum(log(1 + xi * z))\n    out <- out - sum((1 + xi * z)^{-1/xi})\n    \n    priors <- dnorm(par[1], mean = 2.4, sd = 1, log = T) + \n        dnorm(par[2], mean = -0.5, sd = 1, log = T) + \n        dnorm(par[3], mean = 0, sd = 1, log = T) + \n        dnorm(par[4], mean = 0, sd = 1, log = T)\n    \n    out <- out + priors\n    \n    -out\n}\n\n\n\n\nCode\nf_trend <- function(x, t, mu0, sigma, xi, delta) {\n    t <- t - 1981\n    mu <- mu0 * (1 + delta * t)\n    exp(-(1 + xi * (x - mu) / sigma)^{-1/xi})\n}\n\nfit_gev_trend <- function(data, ...) {\n    \n    y <- data$y\n    \n    opt <- optim(log_lik_trend,\n                 par = c(0, 0, 0.1, 0), \n                 dat = data,\n                 hessian = T)\n    \n    par <- opt$par\n    mu <- exp(par[1])\n    sigma <- exp(par[2]) * mu\n    xi <- 1 / (1 + exp(-par[3])) - 0.5\n    delta <- 0.016 * 1 / (1 + exp(-par[4])) - 0.008\n    \n    hess <- opt$hessian \n    \n    tibble(hess = list(hess),\n           par = list(par))\n    \n}\n\n\n\n\nCode\nd_fit <- d |> \n    filter(proj_x <= 10, proj_y <= 10) |>\n    select(year, y = precip, station, proj_x, proj_y) |> \n    group_by(station, proj_x, proj_y) |> \n    group_modify(fit_gev_trend) |> \n    ungroup()\n\n\n\n\nCode\neta_hat <- Matrix(unlist(d_fit$par), ncol = 1)\nQ_etay <- bdiag(d_fit$hess)\nchol_Q_etay <- bdiag(d_fit$hess |> map(chol))\n\ndim(eta_hat)\n\n\n[1] 400   1\n\n\nCode\ndim(Q_etay)\n\n\n[1] 400 400\n\n\nCode\ndim(chol_Q_etay)\n\n\n[1] 400 400\n\n\n\n\nCode\neta <- Matrix(0,\n              nrow = nrow(eta_hat),\n              ncol = 1)\nZ <- Matrix(data = 0, \n            nrow = nrow(eta_hat),\n            ncol = 4)\n\nfor (i in 1:4) Z[i + 0:99 * 4, i] <- 1\n\nnu <- Matrix(data = 0,\n             nrow = 4,\n             ncol = 1)"
  },
  {
    "objectID": "Projects/Modeling/Max & Smooth/index.html#test-run",
    "href": "Projects/Modeling/Max & Smooth/index.html#test-run",
    "title": "1  Max & Smooth",
    "section": "1.3 Test run",
    "text": "1.3 Test run\n\n\nCode\npriors <- d_fit |> \n    unnest(par) |> \n    group_by(station) |> \n    mutate(term = row_number()) |> \n    ungroup() |> \n    group_by(term) |> \n    summarise(mean = mean(par),\n              sd = sd(par),\n              .groups = \"drop\")\n\n\n\n\nCode\ny <- eta_hat\nx <- nu\n\nQ_e <- Q_etay\n\nZ <- Matrix(data = 0, \n            nrow = nrow(eta_hat),\n            ncol = 4)\n\nfor (i in 1:4) Z[i + 0:99 * 4, i] <- 1\n\nmu_x <- Matrix(priors$mean)\nQ_x <- .sparseDiagonal(n = nrow(mu_x), x = 1 / priors$sd^2)\nchol_Q_x <- Cholesky(Q_x)\n\n\nyx <- rbind(y, x)\n\nQ_yx_cond_theta <- Matrix(data = 0,\n                          nrow = nrow(y) + nrow(x),\n                          ncol = nrow(y) + nrow(x))\n\n# Upper left of matrix\nQ_yx_cond_theta[1:nrow(y), 1:nrow(y)] <- Q_e\n# Upper right of matrix\nQ_yx_cond_theta[1:nrow(y), (nrow(y) + 1):(nrow(y) + nrow(x))] <- -Q_e %*% Z\n# Lower left of matrix\nQ_yx_cond_theta[(nrow(y) + 1):(nrow(y) + nrow(x)), 1:nrow(y)] <- - t(Z) %*% Q_e\n# Lower right of matrix\nQ_yx_cond_theta[(nrow(y) + 1):(nrow(y) + nrow(x)), (nrow(y) + 1):(nrow(y) + nrow(x))] <- Q_x + t(Z) %*% Q_e %*% Z\n\nchol_Q_yx_cond_theta <- Cholesky(Q_yx_cond_theta)\n\n\nQ_x_cond_y <- Q_x + t(Z) %*% Q_e %*% Z\nmu_x_cond_y <- solve(Q_x_cond_y, Q_x %*% mu_x + t(Z) %*% Q_e %*% y)"
  },
  {
    "objectID": "Projects/Modeling/Max & Smooth/index.html#sampling-from-the-posterior",
    "href": "Projects/Modeling/Max & Smooth/index.html#sampling-from-the-posterior",
    "title": "1  Max & Smooth",
    "section": "1.4 Sampling from the posterior",
    "text": "1.4 Sampling from the posterior\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathrm{GEV_{trend}}(\\mu_i, \\sigma_i, \\xi_i, \\Delta_i) \\\\\n(\\psi_i, \\tau_i, \\phi_i, \\gamma_i) &= (\\log(\\mu_i), \\log(\\sigma_i/\\mu_i), h(\\xi_i), d(\\Delta_i)) \\\\\n\\begin{bmatrix}\n\\psi_i \\\\ \\tau_i \\\\ \\phi_i \\\\ \\gamma_i\n\\end{bmatrix} &=\nN\\left(\n\\begin{bmatrix}\n\\mu_\\psi \\\\\n\\mu_\\tau \\\\\n\\mu_\\phi \\\\\n\\mu_\\gamma\n\\end{bmatrix},\n\\mathrm{diag}(\\sigma_\\psi^2, \\sigma_\\tau^2, \\sigma_\\phi^2, \\sigma_\\gamma^2)\n\\right)\n\\end{aligned}\n\\]\n\n1.4.1 1. Sample \\(\\theta\\) from marginal\nOur \\(\\theta\\) now contains eight parameters: \\(\\psi_i\\), \\(\\tau_i\\), \\(\\phi_i\\), \\(\\gamma_i\\), \\(\\sigma_\\psi\\), \\(\\sigma_\\tau\\), \\(\\sigma_\\phi\\) and \\(\\sigma_\\gamma\\).\nLet’s put \\(N(0, 3)\\) priors on the means and \\(\\mathrm{Exp}(1)\\) priors on the standard deviations.\n\n\nCode\ntheta <- c(priors$mean, 1/priors$sd)\npi_theta <- function(theta) {\n    mus <- theta[1:4]\n    sigmas <- theta[5:8]\n    \n    out <- 0\n    for (i in 1:4) {\n        out <- out + dnorm(mus[i], mean = 0, sd = 3, log = T)\n        out <- out + dexp(sigmas[i], rate = 1, log = T)\n    }\n    \n    out\n}\n\npi_theta(theta)\n\n\n[1] -33.4639\n\n\n\\[\n\\pi(\\theta \\vert \\hat\\eta) \\propto \\pi(\\theta) \\frac{\\pi(\\hat\\eta\\vert x, \\theta)\\pi(x\\vert\\theta)}{\\pi(x\\vert\\hat\\eta, \\theta)}\n\\]\nWe need to know three conditional distributions:\n\n1.4.1.1 1. \\(p(x |\\theta, y)\\)\n\\[\np(x | \\theta, y) = N(x | \\mu_{x|y}, Q_{x|y})\n\\]\n\\[\n\\mu_{x|y} = Q^{-1}_{x|y}(Q_x\\mu_x + Z^TQ_\\varepsilon y) \\\\\nQ_{x|y} = Q_x + Z^TQ_\\varepsilon Z\n\\]\nIn code, we get:\n\n\nCode\nQ_x_cond_y <- Q_x + t(Z) %*% Q_e %*% Z\nmu_x_cond_y <- solve(Q_x_cond_y, Q_x %*% mu_x + t(Z) %*% Q_e %*% y)\n\nchol_Q_x_cond_y <- Cholesky(Q_x_cond_y)\n\npi_x_cond_theta_y <- function(x, mu_x_cond_y, chol_Q_x_cond_y) {\n    dmvn.sparse(\n        x = t(x),\n        mu = as.vector(mu_x_cond_y),\n        CH = chol_Q_x_cond_y,\n        prec = TRUE,\n        log = TRUE\n    )\n}\n\npi_x_cond_theta_y(x = x, mu_x_cond_y = mu_x_cond_y, chol_Q_x_cond_y = chol_Q_x_cond_y)\n\n\n[1] -106542.4\n\n\n\n\n1.4.1.2 2. \\(p(y |x, \\theta)\\)\n\\[\np(y | x, \\theta) = N(y | Zx, Q_\\varepsilon^{-1})\n\\]\nIn code, we get:\n\n\nCode\nQ_e <- bdiag(d_fit$hess)\nchol_Q_e <- Cholesky(Q_e)\n\n\npi_y_cond_x_theta <- function(y, Z, x, chol_Q_e) {\n    dmvn.sparse(\n        x = t(y), \n        mu = as.vector(Z %*% x), \n        CH = chol_Q_e,\n        prec = TRUE,\n        log = TRUE\n    )\n}\n\npi_y_cond_x_theta(y = y, Z = Z, x = x, chol_Q_e = chol_Q_e)\n\n\n[1] -105787.3\n\n\n\n\n1.4.1.3 3. \\(p(x | \\theta)\\)\n\\[\np(x | \\theta)  = N(x | \\mu_x, Q_x^{-1})\n\\]\n\n\nCode\npi_x_cond_theta <- function(x, mu_x, chol_Q_x) {\n    dmvn.sparse(x = t(x),\n                mu = as.vector(mu_x),\n                CH = chol_Q_x,\n                prec = TRUE,\n                log = TRUE)\n}\n\npi_x_cond_theta(x = x, mu_x = mu_x, chol_Q_x = chol_Q_x)\n\n\n[1] -659.8415\n\n\n\n\nCode\npi_theta <- function(theta) 1\npi_theta_cond_y <- function(theta, y, x, mu_x, chol_Q_x, Z, chol_Q_e,\n                            mu_x_cond_y, chol_Q_x_cond_y) {\n    pi_theta(theta) * \n        pi_y_cond_x_theta(y = y, Z = Z, x = x, chol_Q_e = chol_Q_e) * \n        pi_x_cond_theta(x = x, mu_x = mu_x, chol_Q_x = chol_Q_x) /\n        pi_x_cond_theta_y(x = x, mu_x_cond_y = mu_x_cond_y, chol_Q_x_cond_y = chol_Q_x_cond_y)\n}\n\npi_theta_cond_y(\n    theta = theta, \n    y = y,\n    x = x, \n    mu_x = mu_x, \n    chol_Q_x = chol_Q_x, \n    Z = Z,\n    chol_Q_e, \n    mu_x_cond_y,\n    chol_Q_x_cond_y = chol_Q_x_cond_y\n)\n\n\n[1] -655.1651\n\n\n\n\n\n1.4.2 2. Sample \\(\\mu_x\\) and \\(Q_x\\) given \\(\\theta\\) and \\(\\hat\\eta\\)\n\n\nCode\nQ_x_cond_y <- Q_x + t(Z) %*% Q_e %*% Z\nchol_Q_x_cond_y <- Cholesky(Q_x_cond_y)\n\nmu_x_cond_y <- solve(Q_x_cond_y, Q_x %*% mu_x + t(Z) %*% Q_e %*% y)\n\n\nrmvn.sparse(\n    n = 1,\n    mu = mu_x_cond_y,\n    CH = chol_Q_x_cond_y,\n    prec = TRUE\n)\n\n\n1 x 4 Matrix of class \"dgeMatrix\"\n         [,1]      [,2]       [,3]      [,4]\n[1,] 2.499038 -1.036788 -0.2767325 0.3364385\n\n\n\n\nCode\nmu_yx_cond_theta <- rbind(Z %*% mu_x_cond_y, mu_x_cond_y)\n\nQ_yx_cond_theta <- Matrix(data = 0,\n                          nrow = nrow(mu_yx_cond_theta),\n                          ncol = nrow(mu_yx_cond_theta))\n\n# Upper left of matrix\nQ_yx_cond_theta[1:nrow(y), 1:nrow(y)] <- Q_e\n# Upper right of matrix\nQ_yx_cond_theta[1:nrow(y), (nrow(y) + 1):(nrow(y) + nrow(x))] <- -Q_e %*% Z\n# Lower left of matrix\nQ_yx_cond_theta[(nrow(y) + 1):(nrow(y) + nrow(x)), 1:nrow(y)] <- - t(Z) %*% Q_e\n# Lower right of matrix\nQ_yx_cond_theta[(nrow(y) + 1):(nrow(y) + nrow(x)), (nrow(y) + 1):(nrow(y) + nrow(x))] <- Q_x + t(Z) %*% Q_e %*% Z\n\nchol_Q_yx_cond_theta <- Cholesky(Q_yx_cond_theta)\n\nsamp <- rmvn.sparse(\n    n = 400,\n    mu = mu_yx_cond_theta,\n    CH = chol_Q_yx_cond_theta,\n    prec = TRUE\n) |> \n    as.matrix() |> \n    as.data.frame() |> \n    set_names(\n        c(\n            str_c(\n                rep(c(\"psi\", \"tau\", \"phi\", \"lambda\"), times = 100), rep(1:100, each = 4)\n                ),\n            \"mu_psi\", \"mu_tau\", \"mu_phi\", \"mu_lambda\"\n        )\n    ) |> \n    as_tibble() |> \n    mutate(iter = row_number()) |> \n    pivot_longer(c(-iter))\n\n\n\nsamp |> \n    filter(str_detect(name, \"[0-9]\")) |> \n    mutate(station = parse_number(name),\n           name = str_replace_all(name, \"[0-9]\", \"\")) |> \n    filter(station %in% sample(unique(station), size = 5)) |> \n    ggplot(aes(value)) +\n    geom_histogram() +\n    facet_grid(station ~ name, scales = \"free_x\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  }
]